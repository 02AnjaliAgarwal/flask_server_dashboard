# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ImxlZGdIpGqyA1jnd58TgrK8ddU-RlCV
"""

import os

# List files in the Colab environment
os.listdir("/content/")

file_path = "/content/logfile.txt"  # Ensure the correct file path

# Open the file and read the first 5 lines
with open(file_path, "r") as file:
    for i in range(5):  # Print only the first 5 lines
        print(file.readline().strip())

import re

# Open the log file
with open(file_path, "r") as file:
    for line in file:
        # Extract the IP address (first part of the line)
        ip_address = line.split()[0]

        # Extract the timestamp using regex
        timestamp_match = re.search(r"\[(.*?)\]", line)  # Finds text inside [ ... ]
        if timestamp_match:
            timestamp = timestamp_match.group(1)  # Get the matched timestamp
            print(f"IP: {ip_address}, Timestamp: {timestamp}")

import re

# Open the log file
with open("logfile.txt", "r") as file:
    for line in file:
        ip_address = line.split()[0]

        timestamp_match = re.search(r"\[(.*?)\]", line)
        if timestamp_match:
            timestamp = timestamp_match.group(1)
            date_part = timestamp.split(":")[0]  # Extracts '17/May/2015'
            hour_part = timestamp.split(":")[1]  # Extracts '10'

            print(f"Date: {date_part}, Hour: {hour_part}, IP: {ip_address}")

from collections import defaultdict
import re
file_path = "/content/logfile.txt"

ip_counts = defaultdict(int)  # Dictionary to store IP occurrences

# Open the log file
with open(file_path, "r") as file:
    for line in file:
        ip_address = line.split()[0]

        timestamp_match = re.search(r"\[(.*?)\]", line)
        if timestamp_match:
            date_part = timestamp_match.group(1).split(":")[0]  # Extract date

            # Count occurrences of each IP
            ip_counts[ip_address] += 1

  #Sort IPs by occurences (non-increasing order) and take only top 10
top_ips = sorted(ip_counts.items(), key=lambda x: x[1], reverse=True)[:10]


# Print the histogram
print("\nTop 10 IP Addresses that Hit the Server:")

print("-----------------------------------")
print("\nIP Address            Occurrences")
print("-----------------------------------------")

#output is too long because log file has many unique IP addresses.
#Colab truncates the output to the last 5000 lines,
#for ip, count in sorted(ip_counts.items(), key=lambda x: x[1], reverse=True):
#some modifications.....

for ip, count in top_ips:

    print(f"{ip:<20} | {count}")

hour_counts = defaultdict(int)  # Dictionary to store hourly traffic

# Open the log file
with open("logfile.txt", "r") as file:
    for line in file:
        timestamp_match = re.search(r"\[(.*?)\]", line)
        if timestamp_match:
            hour_part = timestamp_match.group(1).split(":")[1]  # Extract hour

            # Count requests per hour
            hour_counts[hour_part] += 1

# Print the histogram
print("\nHour  | Visitors")
print("--------------------")
for hour in sorted(hour_counts.keys()):  # Sorting hours (00 to 23)
    print(f"{hour:<5} | {hour_counts[hour]}")

import matplotlib.pyplot as plt

# Get top 10 IPs for visualization
top_ips = sorted(ip_counts.items(), key=lambda x: x[1], reverse=True)[:10]
ips, counts = zip(*top_ips)  # Unpacking IPs and counts

# Plot
plt.figure(figsize=(10, 5))
plt.bar(ips, counts, color='skyblue')
plt.xlabel("IP Address")
plt.ylabel("Occurrences")
plt.title("Top 10 IP Addresses by Number of Requests")
plt.xticks(rotation=45)  # Rotate for better readability
plt.show()

# Get sorted hourly data
hours, request_counts = zip(*sorted(hour_counts.items()))  # Extract hours and counts

# Plot
plt.figure(figsize=(8, 5))
plt.bar(hours, request_counts, color='orange')
plt.xlabel("Hour of the Day")
plt.ylabel("Number of Requests")
plt.title("Hourly Traffic Distribution")
plt.show()

from collections import defaultdict
import re

file_path = "/content/logfile.txt"  # Ensure correct file path

ip_counts = defaultdict(int)  # Dictionary to store IP occurrences
total_requests = 0  # Total number of requests

# Open the log file
with open(file_path, "r") as file:
    for line in file:
        ip_address = line.split()[0]  # Extract the IP address

        # Extract the timestamp using regex
        timestamp_match = re.search(r"\[(.*?)\]", line)
        if timestamp_match:
            date_part = timestamp_match.group(1).split(":")[0]  # Extract date

            # Count occurrences of each IP
            ip_counts[ip_address] += 1
            total_requests += 1  # Count total requests

# Sort IPs by request count in descending order
sorted_ips = sorted(ip_counts.items(), key=lambda x: x[1], reverse=True)

# Calculate cumulative percentage and find top contributing IPs
ip_85_percent = []
cumulative_requests = 0

for ip, count in sorted_ips:
    cumulative_requests += count
    ip_85_percent.append((ip, count))

    if (cumulative_requests / total_requests) >= 0.85:  # Stop when reaching 85%
        break

# Print only the top IPs contributing to 85% of traffic
print("\nIP Addresses Contributing to 85% of the Traffic:")
print("-------------------------------------------------")
print("IP Address            | Requests")
print("-------------------------------------------------")
for ip, count in ip_85_percent[:20]:  # Show only the **top 20** contributing IPs
    print(f"{ip:<20} | {count}")

print(f"\nTotal Requests: {total_requests}")

hour_counts = defaultdict(int)  # Dictionary to store hourly traffic
total_requests = 0  # Total number of requests

# Open the log file
with open(file_path, "r") as file:
    for line in file:
        timestamp_match = re.search(r"\[(.*?)\]", line)
        if timestamp_match:
            hour_part = timestamp_match.group(1).split(":")[1]  # Extract hour

            # Count requests per hour
            hour_counts[hour_part] += 1
            total_requests += 1  # Count total requests

# Sort hours by request count in descending order
sorted_hours = sorted(hour_counts.items(), key=lambda x: x[1], reverse=True)

# Calculate cumulative percentage and find top contributing hours
hour_70_percent = []
cumulative_requests = 0

for hour, count in sorted_hours:
    cumulative_requests += count
    hour_70_percent.append((hour, count))

    if (cumulative_requests / total_requests) >= 0.70:  # Stop when reaching 70%
        break

# Print the hours contributing to 70% of the traffic
print("\nHours Contributing to 70% of the Traffic:")
print("-------------------------------------------")
print("Hour  | Requests")
print("-------------------------------------------")
for hour, count in hour_70_percent:
    print(f"{hour:<5} | {count}")

print(f"\nTotal Requests: {total_requests}")